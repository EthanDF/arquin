{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57e31a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import pandas as pd\n",
    "from pandas import Series\n",
    "import numpy as np\n",
    "import re\n",
    "import csv\n",
    "from gsheets import Sheets\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b36f31da",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://docs.google.com/spreadsheets/d/1LRbios7yQRo3aqCh0Es2Wiae_dicg_OtL-_yqP-Tb8I/edit#gid=1718343431'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d0350e",
   "metadata": {},
   "source": [
    "## Create resusable lists for reference in data display or collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86cbab90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lists used in the loading of the AAT Metadata\n",
    "terms_header_list = [\"AACR2_FLAG\", \"DISPLAY_DATE\", \"DISPLAY_NAME\", \"DISPLAY_ORDER\", \"END_DATE\", \"HISTORIC_FLAG\", \"OTHER_FLAGS\", \"PREFERRED\", \"START_DATE\", \"SUBJECT_ID\", \"TERM\", \"TERM_ID\", \"VERNACULAR\"]\n",
    "terms_dtype = {'TERM_ID': str, 'SUBJECT_ID' : str}\n",
    "lang_rels_header_list = [\"LANGUAGE_CODE\", \"PREFERRED\", \"SUBJECT_ID\", \"TERM_ID\", \"QUALIFIER\", \"TERM_TYPE\", \"PART_OF_SPEECH\", \"LANG_STAT\"]\n",
    "lang_rel_dtype = {'TERM_ID': str, 'SUBJECT_ID' : str, 'LANGUAGE_CODE': str}\n",
    "narrow_list = ['identifier', 'media', 'subject', 'title', 'subjects', 'SUBJECT_ID_x', 'TERM_ID', 'join_concept', 'BoxId', 'color']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b1b6fa",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cbd59a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run functions.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf4b9a4",
   "metadata": {},
   "source": [
    "## Load the spreadsheet information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbe2937a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if HttpAccessTokenRefreshError, close everything and start over\n",
    "sheets = Sheets.from_files('~/client_secrets.json', '~/storage.json')\n",
    "s = sheets.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26195a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the list of boxes containing metadata\n",
    "\n",
    "box_list = []\n",
    "box_list_sheets = s.sheets.titles()\n",
    "\n",
    "for b in box_list_sheets:\n",
    "    if 'Box' in b:\n",
    "        box_list.append(int(b[4:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f5ad0e",
   "metadata": {},
   "source": [
    "### Consider comma cases\n",
    "\n",
    "given that we'll parse the created Arquin metadata by comma, we need to handle cases where the subject term, aka concept, contains an internal comma (e.g. \"knives, gauge\"). These will need to be parsed carfully for two reasons:\n",
    "\n",
    "1.) The comma could falsely parse the concept\n",
    "\n",
    "2.) The individual components of the concept could be valid AAT terms, too... eg. \"knives, gauge\" Both \"knives\" and \"gauge\" are valid AAT terms while so is \"knives, gauge\". Knowing what the metata analyst intended in this case is highly difficult to determine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b82665de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ethan.fenichel/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3437: DtypeWarning: Columns (4) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "concepts = load_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d90d4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# review the metadata for concepts containing a comma and return those as a list\n",
    "concepts_drop = concepts[concepts['use_concept'].notna()]\n",
    "concepts_search = concepts_drop[concepts_drop['use_concept'].str.contains(',')]\n",
    "concepts_comma = concepts_search['use_concept'].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bda417",
   "metadata": {},
   "source": [
    "## Load the Metadata from Getty AAT\n",
    "\n",
    "Individual AAT Terms can be seen here:\n",
    "https://www.getty.edu/research/tools/vocabularies/aat/\n",
    "\n",
    "We use the .Out files available below for consuming the data - these are batch released so terms in the .Out files may note be availalbe on the AAT search (link above) or vice-versa:\n",
    "http://aatdownloads.getty.edu/\n",
    "\n",
    "Note that when we load files, we join the LANGUAGE_RELS.out and TERM.out files and combine the term and and the qualifier to create the \"subjects\" and \"join_concepts\"\n",
    "\n",
    "The join_concept ensures the case for the term is all lower case\n",
    "\n",
    "We also create a \"full_concept\" but doesn't handle the NULL case well and shouldn't be used for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9f870d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AACR2_FLAG</th>\n",
       "      <th>DISPLAY_DATE</th>\n",
       "      <th>DISPLAY_NAME</th>\n",
       "      <th>DISPLAY_ORDER</th>\n",
       "      <th>END_DATE</th>\n",
       "      <th>HISTORIC_FLAG</th>\n",
       "      <th>OTHER_FLAGS</th>\n",
       "      <th>PREFERRED_x</th>\n",
       "      <th>START_DATE</th>\n",
       "      <th>SUBJECT_ID_x</th>\n",
       "      <th>...</th>\n",
       "      <th>LANGUAGE_CODE</th>\n",
       "      <th>PREFERRED_y</th>\n",
       "      <th>SUBJECT_ID_y</th>\n",
       "      <th>QUALIFIER</th>\n",
       "      <th>TERM_TYPE</th>\n",
       "      <th>PART_OF_SPEECH</th>\n",
       "      <th>LANG_STAT</th>\n",
       "      <th>full_concept</th>\n",
       "      <th>use_concept</th>\n",
       "      <th>join_concept</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>V</td>\n",
       "      <td>NaN</td>\n",
       "      <td>300022903</td>\n",
       "      <td>...</td>\n",
       "      <td>70051</td>\n",
       "      <td>N</td>\n",
       "      <td>300022903</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UF</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "      <td>NaN</td>\n",
       "      <td>knives, gauge</td>\n",
       "      <td>knives, gauge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>P</td>\n",
       "      <td>NaN</td>\n",
       "      <td>300022904</td>\n",
       "      <td>...</td>\n",
       "      <td>70051</td>\n",
       "      <td>P</td>\n",
       "      <td>300022904</td>\n",
       "      <td>NaN</td>\n",
       "      <td>D</td>\n",
       "      <td>PN</td>\n",
       "      <td>U</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hand knives</td>\n",
       "      <td>hand knives</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>V</td>\n",
       "      <td>NaN</td>\n",
       "      <td>300022904</td>\n",
       "      <td>...</td>\n",
       "      <td>70051</td>\n",
       "      <td>N</td>\n",
       "      <td>300022904</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AD</td>\n",
       "      <td>SN</td>\n",
       "      <td>U</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hand knife</td>\n",
       "      <td>hand knife</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>V</td>\n",
       "      <td>NaN</td>\n",
       "      <td>300022904</td>\n",
       "      <td>...</td>\n",
       "      <td>70051</td>\n",
       "      <td>N</td>\n",
       "      <td>300022904</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UF</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "      <td>NaN</td>\n",
       "      <td>knives, hand</td>\n",
       "      <td>knives, hand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>P</td>\n",
       "      <td>NaN</td>\n",
       "      <td>300022905</td>\n",
       "      <td>...</td>\n",
       "      <td>70051</td>\n",
       "      <td>P</td>\n",
       "      <td>300022905</td>\n",
       "      <td>NaN</td>\n",
       "      <td>D</td>\n",
       "      <td>PN</td>\n",
       "      <td>U</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hawkbill knives</td>\n",
       "      <td>hawkbill knives</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483188</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>P</td>\n",
       "      <td>NaN</td>\n",
       "      <td>300430882</td>\n",
       "      <td>...</td>\n",
       "      <td>70051</td>\n",
       "      <td>P</td>\n",
       "      <td>300430882</td>\n",
       "      <td>NaN</td>\n",
       "      <td>D</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "      <td>NaN</td>\n",
       "      <td>prophylactics</td>\n",
       "      <td>prophylactics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483189</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>P</td>\n",
       "      <td>NaN</td>\n",
       "      <td>300434048</td>\n",
       "      <td>...</td>\n",
       "      <td>70051</td>\n",
       "      <td>P</td>\n",
       "      <td>300434048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>D</td>\n",
       "      <td>PN</td>\n",
       "      <td>U</td>\n",
       "      <td>NaN</td>\n",
       "      <td>styptic pencils</td>\n",
       "      <td>styptic pencils</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483190</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>P</td>\n",
       "      <td>NaN</td>\n",
       "      <td>300434776</td>\n",
       "      <td>...</td>\n",
       "      <td>70051</td>\n",
       "      <td>P</td>\n",
       "      <td>300434776</td>\n",
       "      <td>NaN</td>\n",
       "      <td>D</td>\n",
       "      <td>PN</td>\n",
       "      <td>U</td>\n",
       "      <td>NaN</td>\n",
       "      <td>tissues</td>\n",
       "      <td>tissues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483191</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>P</td>\n",
       "      <td>NaN</td>\n",
       "      <td>300431243</td>\n",
       "      <td>...</td>\n",
       "      <td>70051</td>\n",
       "      <td>P</td>\n",
       "      <td>300431243</td>\n",
       "      <td>NaN</td>\n",
       "      <td>D</td>\n",
       "      <td>PN</td>\n",
       "      <td>U</td>\n",
       "      <td>NaN</td>\n",
       "      <td>towelettes</td>\n",
       "      <td>towelettes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483192</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>P</td>\n",
       "      <td>NaN</td>\n",
       "      <td>300432605</td>\n",
       "      <td>...</td>\n",
       "      <td>70051</td>\n",
       "      <td>P</td>\n",
       "      <td>300432605</td>\n",
       "      <td>NaN</td>\n",
       "      <td>D</td>\n",
       "      <td>PN</td>\n",
       "      <td>U</td>\n",
       "      <td>NaN</td>\n",
       "      <td>washcloths</td>\n",
       "      <td>washcloths</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>483188 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       AACR2_FLAG DISPLAY_DATE DISPLAY_NAME  DISPLAY_ORDER END_DATE  \\\n",
       "0             NaN          NaN          NaN            3.0      NaN   \n",
       "1             NaN          NaN          NaN            1.0      NaN   \n",
       "2             NaN          NaN          NaN            2.0      NaN   \n",
       "3             NaN          NaN          NaN            3.0      NaN   \n",
       "4             NaN          NaN          NaN            1.0      NaN   \n",
       "...           ...          ...          ...            ...      ...   \n",
       "483188        NaN          NaN          NaN            1.0      NaN   \n",
       "483189        NaN          NaN          NaN            1.0      NaN   \n",
       "483190        NaN          NaN          NaN            1.0      NaN   \n",
       "483191        NaN          NaN          NaN            1.0      NaN   \n",
       "483192        NaN          NaN          NaN            1.0      NaN   \n",
       "\n",
       "       HISTORIC_FLAG OTHER_FLAGS PREFERRED_x  START_DATE SUBJECT_ID_x  ...  \\\n",
       "0                  C         NaN           V         NaN    300022903  ...   \n",
       "1                  C         NaN           P         NaN    300022904  ...   \n",
       "2                  C         NaN           V         NaN    300022904  ...   \n",
       "3                  C         NaN           V         NaN    300022904  ...   \n",
       "4                  C         NaN           P         NaN    300022905  ...   \n",
       "...              ...         ...         ...         ...          ...  ...   \n",
       "483188             C         NaN           P         NaN    300430882  ...   \n",
       "483189             C         NaN           P         NaN    300434048  ...   \n",
       "483190             C         NaN           P         NaN    300434776  ...   \n",
       "483191             C         NaN           P         NaN    300431243  ...   \n",
       "483192             C         NaN           P         NaN    300432605  ...   \n",
       "\n",
       "       LANGUAGE_CODE PREFERRED_y SUBJECT_ID_y QUALIFIER TERM_TYPE  \\\n",
       "0              70051           N    300022903       NaN        UF   \n",
       "1              70051           P    300022904       NaN         D   \n",
       "2              70051           N    300022904       NaN        AD   \n",
       "3              70051           N    300022904       NaN        UF   \n",
       "4              70051           P    300022905       NaN         D   \n",
       "...              ...         ...          ...       ...       ...   \n",
       "483188         70051           P    300430882       NaN         D   \n",
       "483189         70051           P    300434048       NaN         D   \n",
       "483190         70051           P    300434776       NaN         D   \n",
       "483191         70051           P    300431243       NaN         D   \n",
       "483192         70051           P    300432605       NaN         D   \n",
       "\n",
       "       PART_OF_SPEECH LANG_STAT full_concept      use_concept     join_concept  \n",
       "0                   U         U          NaN    knives, gauge    knives, gauge  \n",
       "1                  PN         U          NaN      hand knives      hand knives  \n",
       "2                  SN         U          NaN       hand knife       hand knife  \n",
       "3                   U         U          NaN     knives, hand     knives, hand  \n",
       "4                  PN         U          NaN  hawkbill knives  hawkbill knives  \n",
       "...               ...       ...          ...              ...              ...  \n",
       "483188              U         U          NaN    prophylactics    prophylactics  \n",
       "483189             PN         U          NaN  styptic pencils  styptic pencils  \n",
       "483190             PN         U          NaN          tissues          tissues  \n",
       "483191             PN         U          NaN       towelettes       towelettes  \n",
       "483192             PN         U          NaN       washcloths       washcloths  \n",
       "\n",
       "[483188 rows x 23 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concepts[concepts['join_concept'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1737bdfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete, imported 69 boxes\n"
     ]
    }
   ],
   "source": [
    "# load all the boxes and split them\n",
    "for i, box_number in enumerate(box_list):\n",
    "    df = return_box(box_number)\n",
    "    clear_output(wait=True)\n",
    "    print(f\"working on Box {box_number}\")\n",
    "    \n",
    "    try:\n",
    "        df = split_df(df)\n",
    "    except:\n",
    "        print(f'failed to split df for box number {box_number}')\n",
    "\n",
    "    if i+1 == 1:\n",
    "        df_combined = df\n",
    "    else:\n",
    "        df_combined = combine_boxes(df_combined, df)\n",
    " \n",
    "clear_output(wait=True)    \n",
    "\n",
    "df = df_combined\n",
    "df['subjects'] = df['subjects'].replace(r'^\\s*$', np.nan, regex=True)\n",
    "df = df.dropna(subset=['subjects'])\n",
    "print(f'complete, imported {i+1} boxes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ac88d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the df with the concepts\n",
    "df = pd.merge(df, concepts, how=\"left\", on='join_concept')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f04f18c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the boxid\n",
    "df['boxid_temp'] = df['media'].str.extract(r\"(_0\\d+_)\")\n",
    "df = df[df['media'].notna()]\n",
    "df['boxid_temp'] = df['boxid_temp'].str.replace(\"_\",\"\")\n",
    "df['boxid_temp'] = pd.to_numeric(df['boxid_temp'], errors='coerce')\n",
    "df = df.dropna(subset=['boxid_temp'])\n",
    "df['boxid_temp'] = df['boxid_temp'].astype('int')\n",
    "df['boxid_temp'] = df['boxid_temp'].astype('str')\n",
    "df['BoxId'] = 'Box '+df['boxid_temp']\n",
    "df.drop('boxid_temp', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9b9330",
   "metadata": {},
   "source": [
    "## Add in Tab Color\n",
    "\n",
    "The colors of each tab are identified in 'get_spreadsheets_color.ipynb'. That script creates a CSV file and that is read in and added to the df_combined object\n",
    "\n",
    "Read in the color match file created here:\n",
    "http://localhost:8888/notebooks/get_spreadsheets.ipynb\n",
    "\n",
    "use that to create the list of titles based on color of the tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be0ab58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_match_df = pd.read_csv('csv_files/color_match', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14e7f629",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, color_match_df, on='BoxId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de232915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the unique media IDs where media is not NA \n",
    "df_medias = df.media.dropna().unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a1d2ae",
   "metadata": {},
   "source": [
    "Run the full flow through in the following cell. Be sure to DELETE the unmatched_subjects_v2.csv file before staring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa7395ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16221 of 16222: search_media = Arquin_069_0217.jpg\n"
     ]
    }
   ],
   "source": [
    "df_medias_len = len(df_medias)\n",
    "\n",
    "for i, search_media in enumerate(df_medias):\n",
    "    clear_output(wait=True) \n",
    "    print(f'{i} of {df_medias_len}: search_media = {search_media}')\n",
    "#     x = input('step 1: press any key to continue... ')\n",
    "    \n",
    "    try_match_neg = None\n",
    "    try_match_pos = None\n",
    "    cancel_repeat = False\n",
    "    concepts_found_pos = []\n",
    "    concepts_found_neg = []\n",
    "    \n",
    "    df_subjects = df[df['media'] == search_media].join_concept.to_list()\n",
    "    df_media_no_match = df[df['media'] == search_media].loc[df['SUBJECT_ID_x'].isna()]\n",
    "#     print(f'{i}: search_media = {search_media}, df_media_no_match = {df_media_no_match}')\n",
    "#     x = input('press any key to continue... ')\n",
    "    \n",
    "    if len(df_media_no_match) > 0:\n",
    "#         print(f'{i}: search_media = {search_media}, passed len(df_media_no_match) > 0 ')\n",
    "#         x = input('step 2: press any key to continue... ')\n",
    "        \n",
    "        \n",
    "        no_match = df_media_no_match.join_concept.to_list()\n",
    "#         print(f'{i}: search_media: {search_media}, no_match = {no_match}')\n",
    "        \n",
    "        for i, unmatched in enumerate(no_match):\n",
    "#             print(f'{i}: unmatched = {unmatched}, remove_con = {remove_con}')\n",
    "#             x = input('step 3: press any key to continue... ')\n",
    "#             if cancel_repeat == True:\n",
    "#                 break\n",
    "            \n",
    "            if i > 0:\n",
    "                if unmatched == remove_this_con:\n",
    "#                     print(f'already removed {remove_con} from  {search_media}')\n",
    "#                     x = input(f'step 4: it worked! Continue? (press x)')\n",
    "#                     cancel_repeat = True\n",
    "                    break\n",
    "            \n",
    "            pos = False\n",
    "            neg = False\n",
    "            new_concept = None\n",
    "            pop_index = None\n",
    "            \n",
    "            no_match_index = df_subjects.index(unmatched)\n",
    "#             print(f'step 5: {i+1}: unmatched concept is: \"{unmatched}\" and its index is \"{no_match_index}\" out of {len(df_subjects)-1}')\n",
    "            \n",
    "            if no_match_index > 0:\n",
    "                try_match_neg = df_subjects[no_match_index-1]+', '+df_subjects[no_match_index]\n",
    "                concepts_found_pos = concepts.loc[concepts['join_concept'] == try_match_neg]\n",
    "                if len(concepts_found_pos) > 0:\n",
    "                    pos = True\n",
    "#                     print(f'pos is {pos}')\n",
    "            if no_match_index < len(df_subjects)-1:\n",
    "                try_match_pos = df_subjects[no_match_index]+', '+df_subjects[no_match_index+1]\n",
    "                concepts_found_neg = concepts.loc[concepts['join_concept'] == try_match_pos]\n",
    "                if len(concepts_found_neg) > 0:\n",
    "                    neg = True\n",
    "#                     print(f'neg is {neg}')\n",
    "#             print(f'try_match_neg = \"{try_match_neg}\"\\ntry_match_pos = \"{try_match_pos}\"')\n",
    "            \n",
    "            try:\n",
    "                if len(concepts_found_pos) == 1:\n",
    "                    new_concept = concepts_found_pos.join_concept.to_list()[0]\n",
    "                    \n",
    "                        \n",
    "            except (NameError):\n",
    "                pass\n",
    "            try:\n",
    "                if len(concepts_found_neg) == 1:\n",
    "                    new_concept = concepts_found_neg.join_concept.to_list()[0]\n",
    "            except (NameError):\n",
    "                pass\n",
    "            try:\n",
    "                if (len(concepts_found_pos) > 0) & (len(concepts_found_neg) > 0):\n",
    "                    print('too many matches!')\n",
    "                    pass\n",
    "            except (NameError):\n",
    "                pass\n",
    "            try:\n",
    "                if (len(concepts_found_pos) == 0) & (len(concepts_found_neg) == 0):\n",
    "                    unmatched_df = df[(df['media'] == search_media) \n",
    "                                      & (df['join_concept'] == unmatched)][narrow_list]\n",
    "                    add_to_not_found(unmatched_df)\n",
    "            except (NameError):\n",
    "                pass\n",
    "            \n",
    "            remove_con = df_subjects[no_match_index]\n",
    "            remove_this_con_index = 0\n",
    "            \n",
    "            if pos:\n",
    "                remove_this_con_index = no_match_index-1\n",
    "            if neg:\n",
    "                remove_this_con_index = no_match_index+1\n",
    "\n",
    "            if pos | neg:\n",
    "                remove_this_con = df_subjects[remove_this_con_index]\n",
    "\n",
    "                df = df.drop(\n",
    "                    df[((df['media'] == search_media) & \n",
    "                                      (df['join_concept'] == remove_this_con)\n",
    "                                     )].index\n",
    "                )\n",
    "            \n",
    "    #     determine the location in the index of the concept to be removed\n",
    "                pop_index = df.loc[(df['media'] == search_media) & (df['join_concept'] == remove_con)].index\n",
    "    #     remove the concept found at that index\n",
    "                df.at[pop_index[0], 'join_concept'] = new_concept\n",
    "                df.at[pop_index[0], 'subjects'] = new_concept\n",
    "                df.at[pop_index[0], 'SUBJECT_ID_x'] = concepts.at[concepts.loc[concepts['join_concept'] == new_concept].index[0], 'SUBJECT_ID_x']\n",
    "                df.at[pop_index[0], 'TERM_ID'] = concepts.at[concepts.loc[concepts['join_concept'] == new_concept].index[0], 'TERM_ID']\n",
    "\n",
    "            print(f'new_concept = {new_concept}')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
